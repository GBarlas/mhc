[ulmfit]
itos_filename = ulmfit/wt103/itos_wt103.pkl
lm_filename = ulmfit/wt103/fwd_wt103_enc.h5


[bert]
model = uncased_L-12_H-768_A-12
do_lower_case = false
layers = -1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12
max_seq_length = 512
batch_size = 32
init_checkpoint = bert/%(model)s/bert_model.ckpt
bert_config_file = bert/%(model)s/bert_config.json
vocab_file = bert/%(model)s/vocab.txt
use_one_hot_embeddings = false


[gpt-2]
model_name = 124M
seed = 5
nsamples = 1
batch_size = 1
length = None
temperature = 1
top_k = 0
top_p = 1
models_dir = gpt-2/gpt-2/models

[elmo]
